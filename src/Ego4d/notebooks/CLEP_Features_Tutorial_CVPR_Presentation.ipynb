{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5ef0b74",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Features Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b84360e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1234)\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import h5py\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NARRATION_JSON_PATH = \"/datasets01/ego4d_track2/v1/annotations/narration.json\"\n",
    "NARR_OUT_DIR = \"/tmp/narrs/\"\n",
    "NARR_META_PATH = os.path.join(NARR_OUT_DIR, \"meta.pt\")\n",
    "FEATURE_DIR = \"/checkpoint/miguelmartin/ego4d_track2_features/full_scale/omnivore_video_swinL\"\n",
    "FEATURES_PER_SECOND = 30 / 16\n",
    "FEATURE_DIM = 1536\n",
    "\n",
    "VIDEO_UIDS = [x.split(\".pt\")[0] for x in os.listdir(FEATURE_DIR) if \"yaml\" not in x]\n",
    "random.shuffle(VIDEO_UIDS)\n",
    "\n",
    "EXAMPLE_VIDEO_UID = VIDEO_UIDS[0]\n",
    "VIDEO_UIDS_EXAMPLE_SET = set(VIDEO_UIDS[0:100])\n",
    "\n",
    "os.makedirs(NARR_OUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f3e793",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Step 1: Prepare Data\n",
    "\n",
    "- Preprocess:\n",
    "   1. Ego4D:\n",
    "       1. *Video Features*: convert to HDF5 file\n",
    "       2. *Narration Features*: extract & save to disk\n",
    "   2. Kinetics400: Extract features from Labels / Videos & save to HDF5\n",
    "       - Labels converted to `\"The person in this video is doing <label>\"`\n",
    "   3. Ego-Charades: Extract features from Labels / Videos & save to HDF5\n",
    "       - Labels will be as-is\n",
    "       \n",
    "- HDF5 to store features\n",
    "- Pickle file (`torch.save` / `torch.load`) to store keys as HDF5 is slow with respect to getting keys\n",
    "\n",
    "NOTE: we're not storing narration embeddings/features to HDF5 as with 5million potential narrations to use, you will require to distribute (across many processes or machines) the writes to disk as otherwise it will take a long time to save them all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e952a28c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41cec48e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "FEATURE_HDF5_OUT_PATH = \"features_ex.hdf5\"\n",
    "NARR_HDF5_OUT_PATH = \"narrs_ex.hdf5\"\n",
    "NARR_META_OUT_PATH = \"narrs_ex.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc1cf447",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a72fdcabbfe14ccdb0fdd3a353c2c331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "video_uid:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with h5py.File(FEATURE_HDF5_OUT_PATH, \"w\") as out_f:\n",
    "    for uid in tqdm(VIDEO_UIDS_EXAMPLE_SET, desc=\"video_uid\", leave=True):\n",
    "        feature_path = os.path.join(FEATURE_DIR, f\"{uid}.pt\")\n",
    "        fv = torch.load(feature_path)\n",
    "        out_f.create_dataset(uid, data=fv.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fa5ff8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Narrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "035aacbc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637d5936b2944e75a0061403b7f0879a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f6de59b86914a0e9024067075450de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "50206"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: this is missing validation set removal\n",
    "uid_subset = VIDEO_UIDS_EXAMPLE_SET\n",
    "narration_json = json.load(open(NARRATION_JSON_PATH))\n",
    "narrations = [\n",
    "    (uid, data[\"narration_text\"], data[\"timestamp_sec\"], 1)\n",
    "    for uid in tqdm(uid_subset)\n",
    "    for data in narration_json[uid].\n",
    "    get(\"narration_pass_1\", \n",
    "        {\"narrations\": []})[\"narrations\"]\n",
    "]\n",
    "narrations += [\n",
    "    (uid, data[\"narration_text\"], data[\"timestamp_sec\"], 2)\n",
    "    for uid in tqdm(uid_subset)\n",
    "    for data in narration_json[uid].get(\"narration_pass_2\", {\"narrations\": []})[\"narrations\"]\n",
    "]\n",
    "\n",
    "narrations.sort(key=lambda x: (x[0], x[-1]))\n",
    "len(narrations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0f7ae1d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def sub_tagged_tokens(text: str) -> str:\n",
    "    text = text.replace(\"#C\", \"Camera wearer\")\n",
    "    text = text.replace(\"#O\", \"Other person\")\n",
    "    text = text.replace(\"#unsure\", \"something\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef52aaa7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.33861708641052"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_narrations():\n",
    "    model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "    return model.encode([\n",
    "        sub_tagged_tokens(txt)\n",
    "        for _, txt, _, _ in narrations\n",
    "    ])\n",
    "\n",
    "t1 = time.time()\n",
    "fvs = encode_narrations()\n",
    "t2 = time.time()\n",
    "t2 - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d312c77",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6825f97c1dc42918714a6e7aa2ecef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50206 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8.999829292297363"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "for idx, x in tqdm(enumerate(fvs), total=len(fvs)):\n",
    "    torch.save(x, os.path.join(NARR_OUT_DIR, f\"{idx}.pt\"))\n",
    "t2 = time.time()\n",
    "\n",
    "t2 - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bf720ff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'uid': '057bf03d-b337-475c-82a2-79f0b5b6637f',\n",
       " 'txt': '#C C opens a door',\n",
       " 'ts': 1.2841985999999999,\n",
       " 'idx': 0,\n",
       " 'pass': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save off the keys/metadata with torch.save (pickle)\n",
    "narration_metadata = [\n",
    "    {\"uid\": uid, \"txt\": txt, \"ts\": ts, \"idx\": idx, \"pass\": pazz}\n",
    "    for idx, (uid, txt, ts, pazz) in enumerate(narrations)\n",
    "]\n",
    "torch.save(narration_metadata, NARR_META_PATH)\n",
    "narration_metadata[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c267105",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Preprocess in a similar way for Kinetics and Ego-Charades\n",
    "\n",
    "Please refer to the code in:\n",
    "- `ego4d/research/clep/run_preprocess.py`\n",
    "- `ego4d/research/clep/preprocess/kinetics.py`\n",
    "- `ego4d/research/clep/preprocess/ego_charade.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e53d676",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Step 2: Datasets/Dataloaders\n",
    "\n",
    "- For **classification tasks** (zero-shot) we can build a generic dataloader which accepts as input:\n",
    "   - Feature HDF5 path\n",
    "   - list of `[(key, label_dict)]`\n",
    "- For **video (visual) / narration pairs**: we can build a specialized dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938b0ed9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Utility To Get Start/End Index\n",
    "\n",
    "First thing we'll need is to get the features ranging from `[t1, t2]`\n",
    "\n",
    "- `features_per_sec == 30 / 16` (fps of canonical video divided by stride)\n",
    "- `nf` is the number number of features for video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "587c534e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def get_start_end_idx(t1: float, t2: float, feature_per_sec: float, nf: int):\n",
    "    assert t2 >= 0\n",
    "    x1 = min(\n",
    "        max(0, math.floor(t1 * feature_per_sec)),\n",
    "        nf - 1,\n",
    "    )\n",
    "    x2 = min(\n",
    "       math.floor(t2 * feature_per_sec),\n",
    "       nf - 1,\n",
    "    )\n",
    "    assert x2 >= x1\n",
    "    return x1, x2 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ee7111",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Visual / Language Pair Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd17c616",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class Ego4DClipDset(torch.utils.data.Dataset):\n",
    "    def __init__(self, offset_sec=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.features = h5py.File(FEATURE_HDF5_OUT_PATH)\n",
    "        self.metadata = torch.load(NARR_META_PATH)\n",
    "        self.offset_sec = offset_sec\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        meta = self.metadata[idx]\n",
    "        narr_key = meta[\"idx\"]\n",
    "        uid = meta[\"uid\"]\n",
    "        \n",
    "        t = meta[\"ts\"]\n",
    "        t1 = t - self.offset_sec\n",
    "        t2 = t + self.offset_sec\n",
    "        \n",
    "        vid_feat = self.features[uid]\n",
    "        start_idx, end_idx = get_start_end_idx(\n",
    "            t1, t2, FEATURES_PER_SECOND, len(vid_feat)\n",
    "        )\n",
    "        \n",
    "        txt_feat = torch.load(os.path.join(NARR_OUT_DIR, f\"{narr_key}.pt\"))\n",
    "\n",
    "        return {\n",
    "            # Alternatively you could sample a constant number here\n",
    "            \"video\": torch.tensor(vid_feat[start_idx:end_idx]).mean(0),\n",
    "            \"text\": torch.tensor(txt_feat),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bcdb66e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1536]), 50206)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset = Ego4DClipDset(2)\n",
    "dset[25][\"video\"].shape, len(dset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ccf9e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69499b1f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class ClipModel(nn.Module):\n",
    "    def __init__(self, txt_in_f=768, vid_in_f=1536):\n",
    "        super().__init__()\n",
    "        self.visual_proj = nn.Sequential(\n",
    "            nn.Linear(vid_in_f, FEATURE_DIM),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(FEATURE_DIM, FEATURE_DIM)\n",
    "        )\n",
    "        self.text_proj = nn.Sequential(\n",
    "            nn.Linear(txt_in_f, FEATURE_DIM),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(FEATURE_DIM, FEATURE_DIM)\n",
    "        )\n",
    "        self.apply(self.init_weights)\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "    \n",
    "    def init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(module.weight.data, gain=torch.nn.init.calculate_gain('relu'))\n",
    "            module.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ve = self.visual_proj(x[\"video\"])\n",
    "        te = self.text_proj(x[\"text\"])\n",
    "        return ve, te, self.logit_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cf72a32",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def compute_loss(vid2txt, txt2vid, device):\n",
    "    N = v_f.shape[0]\n",
    "    label = torch.eye(N, device=device)\n",
    "    loss = (\n",
    "        F.cross_entropy(vid2txt, label) +\n",
    "        F.cross_entropy(txt2vid, label)\n",
    "    ) / 2.0\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd579291",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model = ClipModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bf98e11",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dloader = DataLoader(dset, batch_size=128, num_workers=10, pin_memory=False)  # use workers > 1 for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8d415e3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "optim = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.0001,\n",
    "    betas=(0.98, 0.9),\n",
    "    eps=1e-6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20b4d39b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 4.089771747589111, Examples/s: 9071.56106958311\n",
      "Epoch 1, Loss: 3.6496753692626953, Examples/s: 11182.034646274062\n",
      "Epoch 2, Loss: 3.5249929428100586, Examples/s: 11152.022603662079\n",
      "Epoch 3, Loss: 3.4897658824920654, Examples/s: 11082.790540068292\n",
      "Epoch 4, Loss: 3.4889583587646484, Examples/s: 11138.286295208198\n",
      "Epoch 5, Loss: 3.4732935428619385, Examples/s: 11052.156029850274\n",
      "Epoch 6, Loss: 3.3928422927856445, Examples/s: 11061.04516618785\n",
      "Epoch 7, Loss: 3.3515167236328125, Examples/s: 11080.060260427097\n",
      "Epoch 8, Loss: 3.309297561645508, Examples/s: 11108.393463083657\n",
      "Epoch 9, Loss: 3.2793796062469482, Examples/s: 11134.949795987173\n",
      "Epoch 10, Loss: 3.2377395629882812, Examples/s: 11111.494778770793\n",
      "Epoch 11, Loss: 3.257960319519043, Examples/s: 11183.151065605582\n",
      "Epoch 12, Loss: 3.3925580978393555, Examples/s: 11013.553845665432\n",
      "Epoch 13, Loss: 3.3396925926208496, Examples/s: 11161.99962959284\n",
      "Epoch 14, Loss: 3.4016289710998535, Examples/s: 11162.724453935993\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "model.train()\n",
    "for i in range(num_epochs):\n",
    "    n_ex = 0\n",
    "    t1 = time.time()\n",
    "    for batch in dloader:\n",
    "        # xfer to devices\n",
    "        batch = {x: y.to(device) for x, y in batch.items()}\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        \n",
    "        v_f, t_f, logit_scale = model(batch)\n",
    "        vid2txt = logit_scale * v_f @ t_f.T\n",
    "        txt2vid = logit_scale * t_f @ v_f.T\n",
    "        loss = compute_loss(vid2txt, txt2vid, device)\n",
    "        \n",
    "        loss.backward(); optim.step(); # scheduler.step()\n",
    "        n_ex += batch[\"video\"].shape[0]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.logit_scale.clamp_(0, math.log(100))\n",
    "    t2 = time.time()\n",
    "    print(f\"Epoch {i}, Loss: {loss}, Examples/s: {n_ex/(t2 - t1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcb31a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Future Directions\n",
    "\n",
    "- Other paper's directions\n",
    "  - Hard-negative mining or include hard negatives in batch as done in VideoCLIP / EgoVLP\n",
    "  - Heuristic for positive examples (text similarity, EgoVLP heuristic)\n",
    "- Augment training with image dataset (due to omnivore)\n",
    "- Self-attention for the model / feature transformation\n",
    "- Extend to end-to-end training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ego_clip",
   "language": "python",
   "name": "ego_clip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
